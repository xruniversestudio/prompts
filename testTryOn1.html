<!DOCTYPE html>
<html dir="rtl" lang="ar">
<head>
    <title>WebAR تجربة نظارات مع MediaPipe</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- MediaPipe Vision Task -->
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js"></script>
    
    <!-- Basic styling for the page -->
    <style>
        body {
            font-family: 'Cairo', sans-serif;
            margin: 0;
            overflow: hidden;
        }
        canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
        #video, #outputCanvas {
            transform: scaleX(-1); /* Mirror the video and canvas */
        }
        /* Simple spinner for loading state */
        .spinner {
            border: 4px solid rgba(255, 255, 255, 0.3);
            border-radius: 50%;
            border-top: 4px solid #fff;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="bg-gray-900 text-white">
    <!-- Container for video and canvas to overlap -->
    <div id="container" class="relative w-full h-full">
        <video id="video" class="absolute inset-0 w-full h-full object-cover" autoplay playsinline></video>
        <canvas id="outputCanvas" class="absolute inset-0 w-full h-full"></canvas>
    </div>


    <!-- UI Container -->
    <div id="ui-container" class="absolute top-0 left-0 w-full h-full z-10 flex flex-col justify-center items-center p-4 bg-gray-900 bg-opacity-80">
        <!-- Start Button -->
        <div id="start-screen">
            <h1 class="text-4xl font-bold mb-4">تجربة النظارات الافتراضية</h1>
            <p class="text-lg mb-8">اضغط للبدء والسماح بالوصول إلى الكاميرا.</p>
            <button id="startButton" class="bg-purple-600 hover:bg-purple-700 text-white font-bold py-4 px-8 rounded-full shadow-lg transform hover:scale-105 transition-transform duration-300 ease-in-out">
                ابدأ التجربة
            </button>
        </div>

        <!-- Loading/Error message -->
        <div id="loadingMessage" class="hidden bg-black bg-opacity-50 rounded-lg p-6 text-center">
            <div class="spinner mx-auto mb-4"></div>
            <h2 class="text-2xl font-bold mb-2">جاري التحميل...</h2>
            <p id="loading-text">يرجى الانتظار والسماح بالوصول إلى الكاميرا عند الطلب.</p>
        </div>
    </div>

    <!-- Load Three.js -->
    <script src="https://cdn.jsdelivr.net/npm/three@0.153.0/build/three.min.js"></script>
    
    <!-- Main application logic -->
    <script type="module">
        import { FaceLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js";

        const videoElement = document.getElementById('video');
        const canvasElement = document.getElementById('outputCanvas');
        const uiContainer = document.getElementById('ui-container');
        const startScreen = document.getElementById('start-screen');
        const loadingMessage = document.getElementById('loadingMessage');
        const loadingText = document.getElementById('loading-text');
        const startButton = document.getElementById('startButton');
        
        let faceLandmarker;
        let runningMode = "VIDEO";
        let glasses;
        let scene, camera, renderer;

        async function createFaceLandmarker() {
            loadingText.textContent = "جاري تحميل نماذج الذكاء الاصطناعي...";
            const filesetResolver = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
            );
            faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
                baseOptions: {
                    modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
                    delegate: "GPU"
                },
                outputFaceBlendshapes: false,
                outputFacialTransformationMatrixes: true,
                runningMode: runningMode,
                numFaces: 1
            });
            console.log("Face Landmarker model loaded.");
        }

        async function setupCamera() {
            loadingText.textContent = "في انتظار السماح بالوصول إلى الكاميرا...";
            const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' } });
            videoElement.srcObject = stream;
            videoElement.addEventListener("loadeddata", predictWebcam);
        }

        function initThreeScene() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(63, window.innerWidth / window.innerHeight, 0.1, 1000);
            renderer = new THREE.WebGLRenderer({
                canvas: canvasElement,
                alpha: true
            });
            renderer.setPixelRatio(window.devicePixelRatio);
            renderer.setSize(window.innerWidth, window.innerHeight);
            
            // **FIX:** Prevent Three.js from clearing the canvas, allowing the video to show through.
            renderer.autoClear = false;

            // Create the glasses geometry
            const glassesGroup = new THREE.Group();
            const lensMaterial = new THREE.MeshPhysicalMaterial({
                color: 0x111111,
                transmission: 0.9,
                roughness: 0.1,
                ior: 1.5,
                thickness: 0.1,
                specularIntensity: 1,
                clearcoat: 1
            });

            const frameMaterial = new THREE.MeshStandardMaterial({
                color: 0x222222,
                metalness: 0.8,
                roughness: 0.2
            });

            const lensFrameGeometry = new THREE.TorusGeometry(0.07, 0.01, 16, 100);
            const leftFrame = new THREE.Mesh(lensFrameGeometry, frameMaterial);
            const rightFrame = new THREE.Mesh(lensFrameGeometry, frameMaterial);
            leftFrame.position.x = -0.08;
            rightFrame.position.x = 0.08;
            glassesGroup.add(leftFrame, rightFrame);
            
            const lensGeometry = new THREE.CircleGeometry(0.06, 32);
            const leftLens = new THREE.Mesh(lensGeometry, lensMaterial);
            const rightLens = new THREE.Mesh(lensGeometry, lensMaterial);
            leftLens.position.x = -0.08;
            rightLens.position.x = 0.08;
            glassesGroup.add(leftLens, rightLens);

            const bridgeGeometry = new THREE.CylinderGeometry(0.01, 0.01, 0.05, 8);
            const bridge = new THREE.Mesh(bridgeGeometry, frameMaterial);
            bridge.rotation.z = Math.PI / 2;
            glassesGroup.add(bridge);

            glassesGroup.visible = false;
            scene.add(glassesGroup);
            glasses = glassesGroup;

            const ambientLight = new THREE.AmbientLight(0xffffff, 0.8);
            scene.add(ambientLight);
            const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
            directionalLight.position.set(0.5, 1, 1).normalize();
            scene.add(directionalLight);
        }

        let lastVideoTime = -1;
        async function predictWebcam() {
            // **FIX:** Clear the renderer's depth buffer on each frame.
            renderer.clearDepth();

            if (videoElement.currentTime !== lastVideoTime) {
                lastVideoTime = videoElement.currentTime;
                const results = faceLandmarker.detectForVideo(videoElement, performance.now());

                if (results.facialTransformationMatrixes && results.facialTransformationMatrixes.length > 0) {
                    const matrix = results.facialTransformationMatrixes[0].data;
                    glasses.visible = true;
                    glasses.matrix.fromArray(matrix);
                    glasses.matrix.decompose(glasses.position, glasses.quaternion, glasses.scale);
                    glasses.scale.multiplyScalar(0.0011);
                    glasses.position.z -= 0.03;
                } else {
                    glasses.visible = false;
                }
            }
            renderer.render(scene, camera);
            requestAnimationFrame(predictWebcam);
        }

        async function startExperience() {
            startScreen.style.display = 'none';
            loadingMessage.classList.remove('hidden');

            try {
                await createFaceLandmarker();
                await setupCamera();
                initThreeScene();
                // Hide the whole UI container once everything is loaded and running
                uiContainer.style.display = 'none'; 
            } catch (error) {
                console.error("Initialization failed:", error);
                loadingText.innerHTML = `<h2 class="text-2xl font-bold text-red-400">فشل بدء التشغيل</h2><p>${error.message}. يرجى إعادة تحميل الصفحة والمحاولة مرة أخرى.</p>`;
            }
        }

        startButton.addEventListener('click', startExperience);

        window.addEventListener('resize', () => {
             if (renderer && camera) {
                renderer.setSize(window.innerWidth, window.innerHeight);
                camera.aspect = window.innerWidth / window.innerHeight;
                camera.updateProjectionMatrix();
             }
        });
    </script>
</body>
</html>
